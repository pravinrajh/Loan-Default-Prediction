import pandas as pd
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from scipy.sparse import hstack
import missingno as msno
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")

# Load the new dataset
df = pd.read_csv('new_loan_application_data.csv')

# Function to calculate null value percentages
def null_percentage(data):
    null_values_total = data.isnull().sum().sort_values(ascending=False)
    total_null_percent = (data.isnull().sum() / data.isnull().count() * 100).sort_values(ascending=False)
    return pd.concat([null_values_total, total_null_percent], axis=1, keys=['Total', 'Percent'])

print("Dataset shape:", df.shape)

# Drop columns with more than 50% missing values
for column in df.columns:
    if null_percentage(df[column].to_frame())['Percent'][0] > 50.00:
        df.drop([column], axis=1, inplace=True)

# Imputation Functions
def mean_imputation(data):
    return data.fillna(data.mean())

def median_imputation(data):
    return data.fillna(data.median())

def mode_imputation(data):
    return data.fillna(data.mode().iloc[0])

# Apply imputation to numerical features
num_features = df.select_dtypes(include=['int64', 'float64']).columns
df[num_features] = mean_imputation(df[num_features])

# Handle categorical features
cat_features = df.select_dtypes(include=['object']).columns
df[cat_features] = df[cat_features].apply(lambda x: x.fillna(x.mode().iloc[0]))

# Missing values visualization
msno.matrix(df)
plt.show()

# Define input and output variables
X = df.drop(['loan_status', 'applicant_id'], axis=1)  # Removing applicant_id (not needed for prediction)
y = df['loan_status'].map({'Approved': 1, 'Rejected': 0})  # Convert target to binary

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

# One-Hot Encoding for categorical features
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)
X_train_ohe = encoder.fit_transform(X_train[cat_features])
X_test_ohe = encoder.transform(X_test[cat_features])

# Standardizing numerical features
scaler = StandardScaler()
X_train_num = scaler.fit_transform(X_train[num_features])
X_test_num = scaler.transform(X_test[num_features])

# Combine processed categorical and numerical features
X_train_final = hstack([X_train_num, X_train_ohe])
X_test_final = hstack([X_test_num, X_test_ohe])

# Print final dataset shapes
print("Training Data Shape:", X_train_final.shape)
print("Testing Data Shape:", X_test_final.shape)
